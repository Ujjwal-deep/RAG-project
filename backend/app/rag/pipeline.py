"""
pipeline.py

Orchestration for the RAG pipeline (host-ready).

Provides two async functions:
- process_document(file_bytes, filename, document_id, chunk_size, overlap)
    -> ingests file, splits into chunks, embeds, and upserts into Supabase.
- answer_query(question, top_k)
    -> embeds question, retrieves top-k similar chunks, returns (answer_text, sources)

This file purposely does NOT call any LLM yet. It returns retrieved context (sources)
so you can wire an LLM of your choice later in the pipeline.
"""

from typing import List, Tuple, Dict, Any, Optional
import asyncio

from .ingest import extract_text
from .splitter import split_text_to_chunks
from .embeddings import embed_texts
from .vectorstore import upsert_embeddings, query_similar


async def process_document(
    file_bytes: bytes,
    filename: str,
    document_id: str,
    chunk_size: int = 1000,
    overlap: int = 200,
) -> Dict[str, Any]:
    """
    Full pipeline for ingesting a single document.

    Steps:
    1) extract raw text from bytes
    2) split into chunks (list of dicts with chunk_id/text)
    3) embed chunk texts
    4) upsert into Supabase table (idempotent by document_id)

    Returns a summary dict: {"document_id": ..., "num_chunks": n, "inserted": x}
    """

    # 1) extract text (IO / CPU bound) -> run in thread
    text = await asyncio.to_thread(extract_text, file_bytes, filename)

    # 2) split into chunks (CPU bound but small) -> run in thread
    chunk_dicts = await asyncio.to_thread(split_text_to_chunks, text, chunk_size, overlap)
    # convert to simple list of chunk texts ordered by chunk_id
    chunks = [c["text"] for c in chunk_dicts]

    # 3) embed chunks (sentence-transformers; can be heavy) -> run in thread
    embeddings = await asyncio.to_thread(embed_texts, chunks)

    # 4) upsert into supabase (network I/O) -> run in thread
    # optional: attach per-chunk metadata (example: filename)
    metadata = [{"filename": filename} for _ in chunks]
    upsert_result = await asyncio.to_thread(
        upsert_embeddings, document_id, chunks, embeddings, metadata, True
    )

    return {
        "document_id": document_id,
        "num_chunks": len(chunks),
        "inserted": upsert_result.get("inserted", None),
        "requested": upsert_result.get("requested", None),
    }


async def answer_query(question: str, top_k: int = 2) -> Tuple[str, List[Dict[str, Any]]]:
    """
    Embed the question, retrieve top_k similar chunks, and return:
      - answer (generated by Amazon Bedrock)
      - sources (list of dicts returned from query_similar)
    
    Uses Amazon Bedrock Titan model for LLM inference.
    """
    import os
    import json
    import logging
    
    # Validate and cap top_k at maximum of 3
    top_k = min(top_k, 3)
    
    # 1) embed the question
    q_emb_list = await asyncio.to_thread(embed_texts, [question])
    if not q_emb_list:
        return "No embedding produced for the query.", []

    q_emb = q_emb_list[0]

    # 2) query supabase for similar chunks
    hits = await asyncio.to_thread(query_similar, q_emb, top_k)

    # 3) build combined context and sources
    contexts = []
    sources: List[Dict[str, Any]] = []
    for h in hits:
        # expected keys from RPC: id, document_id, chunk_id, chunk_text, score
        chunk_text = h.get("chunk_text") or h.get("text") or ""
        contexts.append(chunk_text)
        sources.append({
            "id": h.get("id"),
            "document_id": h.get("document_id"),
            "chunk_id": h.get("chunk_id"),
            "score": h.get("score"),
            "text": chunk_text[:500]  # truncated preview
        })

    combined_context = "\n\n---\n\n".join(contexts).strip()

    # 4) Call Amazon Bedrock for LLM inference
    if combined_context:
        def _call_bedrock_sync(prompt: str) -> str:
            """Synchronous Bedrock inference call using boto3."""
            import boto3
            from botocore.exceptions import ClientError, BotoCoreError
            
            # Read AWS credentials from environment variables
            aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
            aws_secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
            aws_region = os.getenv("AWS_REGION")
            model_id = os.getenv("BEDROCK_MODEL_ID", "amazon.titan-text-express-v1")
            
            if not aws_access_key_id or not aws_secret_access_key:
                raise RuntimeError("AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY must be set in environment.")
            if not aws_region:
                raise RuntimeError("AWS_REGION must be set in environment.")
            
            # Create Bedrock Runtime client
            bedrock_runtime = boto3.client(
                "bedrock-runtime",
                aws_access_key_id=aws_access_key_id,
                aws_secret_access_key=aws_secret_access_key,
                region_name=aws_region
            )
            
            # Prepare request body for Titan model
            body = json.dumps({
                "inputText": prompt,
                "textGenerationConfig": {
                    "maxTokenCount": 512,
                    "temperature": 0.2,
                    "topP": 0.9
                }
            })
            
            try:
                # Invoke model
                response = bedrock_runtime.invoke_model(
                    modelId=model_id,
                    body=body,
                    contentType="application/json",
                    accept="application/json"
                )
                
                # Parse response
                response_body = json.loads(response["body"].read())
                
                # Extract generated text from Titan response
                if "results" in response_body and len(response_body["results"]) > 0:
                    generated_text = response_body["results"][0].get("outputText", "").strip()
                    return generated_text
                else:
                    raise RuntimeError(f"Unexpected Bedrock response format: {response_body}")
                    
            except ClientError as e:
                error_code = e.response.get("Error", {}).get("Code", "Unknown")
                error_message = e.response.get("Error", {}).get("Message", str(e))
                raise RuntimeError(f"Bedrock ClientError ({error_code}): {error_message}")
            except BotoCoreError as e:
                raise RuntimeError(f"Bedrock BotoCoreError: {str(e)}")
        
        # Construct prompt with explicit instruction
        prompt = (
            "You are a helpful assistant. Use ONLY the provided context to answer the question. "
            "Do not hallucinate. If the answer cannot be found in the provided context, "
            "explicitly state that you cannot answer based on the given context.\n\n"
            f"CONTEXT:\n{combined_context}\n\n"
            f"QUESTION: {question}\n\n"
            "Answer concisely based solely on the provided context."
        )
        
        try:
            # Call Bedrock in a thread so async loop stays responsive
            answer = await asyncio.to_thread(_call_bedrock_sync, prompt)
        except Exception as e:
            # Log error and provide fallback
            logger = logging.getLogger("rag-backend")
            logger.exception("Bedrock inference failed")
            answer = f"Unable to generate answer: {str(e)}"
    else:
        answer = "No relevant context found for the query."

    return answer, sources